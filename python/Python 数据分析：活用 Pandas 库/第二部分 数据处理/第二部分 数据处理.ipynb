{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 4 章：数据组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "--------------------------------------------------\n",
      "    A   B   C   D\n",
      "0  a4  b4  c4  d4\n",
      "1  a5  b5  c5  d5\n",
      "2  a6  b6  c6  d6\n",
      "3  a7  b7  c7  d7\n",
      "--------------------------------------------------\n",
      "     A    B    C    D\n",
      "0   a8   b8   c8   d8\n",
      "1   a9   b9   c9   d9\n",
      "2  a10  b10  c10  d10\n",
      "3  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "# 加载几个数据集，展示如何连接数据集\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "print(df1)\n",
    "print('-' * 50)\n",
    "print(df2)\n",
    "print('-' * 50)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  a4  b4  c4  d4\n"
     ]
    }
   ],
   "source": [
    "# concat示例\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "row_concat = pd.concat([df1, df2, df3])\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D\n",
      "3   a3   b3   c3   d3\n",
      "3   a7   b7   c7   d7\n",
      "3  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "# concat示例\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "row_concat = pd.concat([df1, df2, df3])\n",
    "print(row_concat.loc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    0\n",
      "0   a0   b0   c0   d0  NaN\n",
      "1   a1   b1   c1   d1  NaN\n",
      "2   a2   b2   c2   d2  NaN\n",
      "3   a3   b3   c3   d3  NaN\n",
      "0  NaN  NaN  NaN  NaN   n1\n",
      "1  NaN  NaN  NaN  NaN   n2\n",
      "2  NaN  NaN  NaN  NaN   n3\n",
      "3  NaN  NaN  NaN  NaN   n4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "new_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])    # 新建一行数据\n",
    "a = pd.concat([df1, new_row_series])                    # 尝试向 DataFrame 中添加新行\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "# series加入dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "# 注意使用了两个方括号，这样 shape 是 [1, 4], 一个方括号，shape 是 [4, 1]\n",
    "new_row_df = pd.DataFrame([['n1', 'n2', 'n3', 'n4']],  \n",
    "                        columns=['A', 'B', 'C', 'D'])\n",
    "print(pd.concat([df1, new_row_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  a4  b4  c4  d4\n",
      "1  a5  b5  c5  d5\n",
      "2  a6  b6  c6  d6\n",
      "3  a7  b7  c7  d7\n",
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "4  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "print(df1.append(df2))          # 添加一个dataframe\n",
    "\n",
    "data_dict = {'A': 'n1', 'B': 'n2', 'C': 'n3', 'D': 'n4'}\n",
    "# 使用 Python 字典添加数据行\n",
    "# 需设置参数 ignore_index=True 忽略掉索引\n",
    "print(df1.append(data_dict, ignore_index=True))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A    B    C    D\n",
      "0    a0   b0   c0   d0\n",
      "1    a1   b1   c1   d1\n",
      "2    a2   b2   c2   d2\n",
      "3    a3   b3   c3   d3\n",
      "4    a4   b4   c4   d4\n",
      "5    a5   b5   c5   d5\n",
      "6    a6   b6   c6   d6\n",
      "7    a7   b7   c7   d7\n",
      "8    a8   b8   c8   d8\n",
      "9    a9   b9   c9   d9\n",
      "10  a10  b10  c10  d10\n",
      "11  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "# ignore_index示例\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "row_concat_i = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(row_concat_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   A    A\n",
      "0  a0  a4   a8\n",
      "1  a1  a5   a9\n",
      "2  a2  a6  a10\n",
      "3  a3  a7  a11\n"
     ]
    }
   ],
   "source": [
    "# 按列名取子集\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "col_concat = pd.concat([df1, df2, df3], axis=1)\n",
    "print(col_concat['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D    A    B    C    D new_col_series\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8             n1\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9             n2\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10             n3\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11             n4\n"
     ]
    }
   ],
   "source": [
    "# 添加列series方法\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "col_concat = pd.concat([df1, df2, df3], axis=1)\n",
    "col_concat['new_col_series'] = pd.Series(['n1', 'n2', 'n3', 'n4'])\n",
    "print(col_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n",
      "     A    C\n",
      "0   a0   c0\n",
      "1   a1   c1\n",
      "2   a2   c2\n",
      "3   a3   c3\n",
      "0   a8   b8\n",
      "1   a9   b9\n",
      "2  a10  b10\n",
      "3  a11  b11\n"
     ]
    }
   ],
   "source": [
    "# 连接具有不同列的行\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "# 调整 DataFrame 的列\n",
    "df1.columns = ['A', 'B', 'C', 'D']\n",
    "df2.columns = ['E', 'F', 'G', 'H']\n",
    "df3.columns = ['A', 'C', 'F', 'H']\n",
    "\n",
    "# 只保留共有的列，使用 join 参数实现\n",
    "print(pd.concat([df1, df2, df3], join='inner')) # 此处会返回空，因为上面3个DataFrame 无共有列\n",
    "\n",
    "# 连接 DataFrame 有相同的列\n",
    "print(pd.concat([df1, df3], ignore_index=False, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D\n",
      "0  a0  b0  c0  d0  a8  b8  c8  d8\n",
      "2  a2  b2  c2  d2  a9  b9  c9  d9\n"
     ]
    }
   ],
   "source": [
    "# 连接具有不同行的列\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../pandas_for_everyone-master/data/concat_1.csv')\n",
    "df2 = pd.read_csv('../pandas_for_everyone-master/data/concat_2.csv')\n",
    "df3 = pd.read_csv('../pandas_for_everyone-master/data/concat_3.csv')\n",
    "\n",
    "# 调整 DataFrame 的行索引\n",
    "df1.index = [0, 1, 2, 3]\n",
    "df2.index = [4, 5, 6, 7]\n",
    "df3.index = [0, 2, 5, 7]\n",
    "\n",
    "# 只保留索引匹配的结果，参数 join='inner'\n",
    "print(pd.concat([df1, df3], axis=1, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ident   personal    family\n",
      "0      dyer    William      Dyer\n",
      "1        pb      Frank   Pabodie\n",
      "2      lake   Anderson      Lake\n",
      "3       roe  Valentina   Roerich\n",
      "4  danforth      Frank  Danforth\n",
      "    name    lat    long\n",
      "0   DR-1 -49.85 -128.57\n",
      "1   DR-3 -47.15 -126.72\n",
      "2  MSK-4 -48.87 -123.40\n",
      "    taken person quant  reading\n",
      "0     619   dyer   rad     9.82\n",
      "1     619   dyer   sal     0.13\n",
      "2     622   dyer   rad     7.80\n",
      "3     622   dyer   sal     0.09\n",
      "4     734     pb   rad     8.41\n",
      "5     734   lake   sal     0.05\n",
      "6     734     pb  temp   -21.50\n",
      "7     735     pb   rad     7.22\n",
      "8     735    NaN   sal     0.06\n",
      "9     735    NaN  temp   -26.00\n",
      "10    751     pb   rad     4.35\n",
      "11    751     pb  temp   -18.50\n",
      "12    751   lake   sal     0.10\n",
      "13    752   lake   rad     2.19\n",
      "14    752   lake   sal     0.09\n",
      "15    752   lake  temp   -16.00\n",
      "16    752    roe   sal    41.60\n",
      "17    837   lake   rad     1.46\n",
      "18    837   lake   sal     0.21\n",
      "19    837    roe   sal    22.50\n",
      "20    844    roe   rad    11.25\n",
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n"
     ]
    }
   ],
   "source": [
    "# 预览几个数据集\n",
    "import pandas as pd\n",
    "\n",
    "person = pd.read_csv('../pandas_for_everyone-master/data/survey_person.csv')\n",
    "site = pd.read_csv('../pandas_for_everyone-master/data/survey_site.csv')\n",
    "survey = pd.read_csv('../pandas_for_everyone-master/data/survey_survey.csv')\n",
    "visited = pd.read_csv('../pandas_for_everyone-master/data/survey_visited.csv')\n",
    "\n",
    "print(person)\n",
    "print(site)\n",
    "print(survey)\n",
    "print(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "2  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "# 一对一合并\n",
    "# 修改 visited DataFrame 使其不含重复的 site 值\n",
    "visited_subset = visited.loc[[0, 2, 6], ]\n",
    "\n",
    "# merge 函数的参数 how 默认值为 inner\n",
    "# 这里无需专门指定\n",
    "o2o_merge = site.merge(visited_subset, left_on='name', right_on='site')\n",
    "print(o2o_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10\n",
      "2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22\n",
      "3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12\n",
      "5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26\n",
      "6   DR-3 -47.15 -126.72    752   DR-3         NaN\n",
      "7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "m2o_merge = site.merge(visited, left_on='name', right_on='site')\n",
    "print(m2o_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ident_x personal   family  taken_x person_x quant  reading  ident_y  site  \\\n",
      "0    dyer  William     Dyer      619     dyer   rad     9.82      619  DR-1   \n",
      "1    dyer  William     Dyer      619     dyer   sal     0.13      619  DR-1   \n",
      "2    dyer  William     Dyer      622     dyer   rad     7.80      622  DR-1   \n",
      "3    dyer  William     Dyer      622     dyer   sal     0.09      622  DR-1   \n",
      "4      pb    Frank  Pabodie      734       pb   rad     8.41      734  DR-3   \n",
      "\n",
      "        dated  taken_y person_y  \n",
      "0  1927-02-08      619     dyer  \n",
      "1  1927-02-08      619     dyer  \n",
      "2  1927-02-10      622     dyer  \n",
      "3  1927-02-10      622     dyer  \n",
      "4  1939-01-07      734       pb  \n"
     ]
    }
   ],
   "source": [
    "# 合并生成 ps、vs 两个 DataFrame\n",
    "ps = person.merge(survey, left_on='ident', right_on='person')\n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# 将要匹配的多列，以列表形式传入 merge 函数\n",
    "ps_vs = ps.merge(vs, left_on=['ident', 'taken', 'quant', 'reading'], right_on=['person', 'ident', 'quant', 'reading'])\n",
    "print(ps_vs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 5 章：缺失数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 从 NumPy 导入缺失值\n",
    "from numpy import NaN, NAN, nan\n",
    "\n",
    "print(NaN == True)      # 缺失值不等于 True，输出 False\n",
    "print(NaN == False)     # 缺失值也不是 False，输出 False\n",
    "print(NaN == 0)         # 缺失值也不是 0，输出 False\n",
    "print(NaN == '')        # 缺失值也不是 空字符串，输出 False\n",
    "print(NaN == NaN)       # 一个缺失值也不等于另一个缺失值，输出 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# isnull, notnull 测试缺失值\n",
    "import pandas as pd\n",
    "\n",
    "print(pd.isnull(NaN))           # NaN 为缺失值，输出 True\n",
    "print(pd.isnull(nan))           # nan 为缺失值，输出 True\n",
    "print(pd.isnull(NAN))           # NAN 为缺失值，输出 True\n",
    "\n",
    "print(pd.notnull(NaN))          # NaN 为缺失值，输出 False\n",
    "print(pd.notnull(42))           # 42 为非缺失值，输出 True\n",
    "print(pd.notnull('message'))    # 'message' 为非缺失值，输出 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n",
      "--------------------------------------------------\n",
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3            \n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n",
      "--------------------------------------------------\n",
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n"
     ]
    }
   ],
   "source": [
    "# 指定数据位置\n",
    "visited_file = '../pandas_for_everyone-master/data/survey_visited.csv'\n",
    "\n",
    "# 使用默认值加载数据\n",
    "print(pd.read_csv(visited_file))\n",
    "print('-' * 50)\n",
    "\n",
    "# 加载数据，不包含默认缺失值\n",
    "print(pd.read_csv(visited_file, keep_default_na=False))\n",
    "print('-' * 50)\n",
    "\n",
    "# 手动指定缺失值\n",
    "print(pd.read_csv(visited_file, na_values=[''], keep_default_na=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ident   site       dated  taken person quant  reading\n",
      "0     619   DR-1  1927-02-08    619   dyer   rad     9.82\n",
      "1     619   DR-1  1927-02-08    619   dyer   sal     0.13\n",
      "2     622   DR-1  1927-02-10    622   dyer   rad     7.80\n",
      "3     622   DR-1  1927-02-10    622   dyer   sal     0.09\n",
      "4     734   DR-3  1939-01-07    734     pb   rad     8.41\n",
      "5     734   DR-3  1939-01-07    734   lake   sal     0.05\n",
      "6     734   DR-3  1939-01-07    734     pb  temp   -21.50\n",
      "7     735   DR-3  1930-01-12    735     pb   rad     7.22\n",
      "8     735   DR-3  1930-01-12    735    NaN   sal     0.06\n",
      "9     735   DR-3  1930-01-12    735    NaN  temp   -26.00\n",
      "10    751   DR-3  1930-02-26    751     pb   rad     4.35\n",
      "11    751   DR-3  1930-02-26    751     pb  temp   -18.50\n",
      "12    751   DR-3  1930-02-26    751   lake   sal     0.10\n",
      "13    752   DR-3         NaN    752   lake   rad     2.19\n",
      "14    752   DR-3         NaN    752   lake   sal     0.09\n",
      "15    752   DR-3         NaN    752   lake  temp   -16.00\n",
      "16    752   DR-3         NaN    752    roe   sal    41.60\n",
      "17    837  MSK-4  1932-01-14    837   lake   rad     1.46\n",
      "18    837  MSK-4  1932-01-14    837   lake   sal     0.21\n",
      "19    837  MSK-4  1932-01-14    837    roe   sal    22.50\n",
      "20    844   DR-1  1932-03-22    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "# 合并数据产生缺失值\n",
    "visited = pd.read_csv('../pandas_for_everyone-master/data/survey_visited.csv')\n",
    "survey = pd.read_csv('../pandas_for_everyone-master/data/survey_survey.csv')\n",
    "\n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken')   \n",
    "print(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goat      4.0\n",
      "amoeba    NaN\n",
      "dtype: float64\n",
      "--------------------------------------------------\n",
      "                Name    Occupation        Born        Died  missing\n",
      "0  Rosaline Franklin       Chemist  1920-07-25  1958-04-16      NaN\n",
      "1     William Gosset  Statistician  1876-06-13  1937-10-16      NaN\n",
      "--------------------------------------------------\n",
      "                Name    Occupation        Born        Died  missing\n",
      "0  Rosaline Franklin       Chemist  1920-07-25  1958-04-16      NaN\n",
      "1     William Gosset  Statistician  1876-06-13  1937-10-16      NaN\n"
     ]
    }
   ],
   "source": [
    "# Series 包含缺失值\n",
    "num_legs = pd.Series({'goat': 4, 'amoeba': nan})\n",
    "print(num_legs)\n",
    "print('-' * 50)\n",
    "\n",
    "# DataFrame 包含缺失值\n",
    "scientists = pd.DataFrame({\n",
    "    'Name': ['Rosaline Franklin', 'William Gosset'],\n",
    "    'Occupation': ['Chemist', 'Statistician'],\n",
    "    'Born': ['1920-07-25', '1876-06-13'],\n",
    "    'Died': ['1958-04-16', '1937-10-16'],\n",
    "    'missing': [NaN, nan]\n",
    "})\n",
    "print(scientists)\n",
    "print('-' * 50)\n",
    "\n",
    "# 直接把一列缺失值赋给 DataFrame\n",
    "scientists = scientists = pd.DataFrame({\n",
    "    'Name': ['Rosaline Franklin', 'William Gosset'],\n",
    "    'Occupation': ['Chemist', 'Statistician'],\n",
    "    'Born': ['1920-07-25', '1876-06-13'],\n",
    "    'Died': ['1958-04-16', '1937-10-16'],\n",
    "})\n",
    "\n",
    "scientists['missing'] = nan\n",
    "print(scientists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "1952    49.057620\n",
      "1957    51.507401\n",
      "1962    53.609249\n",
      "1967    55.678290\n",
      "1972    57.647386\n",
      "1977    59.570157\n",
      "1982    61.533197\n",
      "1987    63.212613\n",
      "1992    64.160338\n",
      "1997    65.014676\n",
      "2002    65.694923\n",
      "2007    67.007423\n",
      "Name: lifeExp, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gapminder = pd.read_csv('../pandas_for_everyone-master/data/gapminder.tsv', sep='\\t')\n",
    "\n",
    "life_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n",
    "print(life_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-155165066461>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlife_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgapminder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lifeExp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlife_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2002\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1759\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1761\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1762\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1763\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[1;31m# ugly hack for GH #836\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take_opportunity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[1;31m# no shortcut needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         d = {\n\u001b[0;32m   1338\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m         }\n\u001b[0;32m   1341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_with_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         d = {\n\u001b[0;32m   1338\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m         }\n\u001b[0;32m   1341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_with_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1552\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m         )\n\u001b[0;32m   1554\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1652\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1653\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 1654\u001b[1;33m                     \u001b[1;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1655\u001b[0m                     \u001b[1;34m\"is no longer supported, see \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                     \u001b[1;34m\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"\u001b[0m  \u001b[1;31m# noqa:E501\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'"
     ]
    }
   ],
   "source": [
    "# 加入 loc 进行切片\n",
    "gapminder = pd.read_csv('../pandas_for_everyone-master/data/gapminder.tsv', sep='\\t')\n",
    "\n",
    "life_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n",
    "print(life_exp.loc[[2000, 2002], ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2002    65.694923\n",
      "2007    67.007423\n",
      "Name: lifeExp, dtype: float64\n",
      "year\n",
      "2000          NaN\n",
      "2001          NaN\n",
      "2002    65.694923\n",
      "2003          NaN\n",
      "2004          NaN\n",
      "2005          NaN\n",
      "2006          NaN\n",
      "2007    67.007423\n",
      "2008          NaN\n",
      "2009          NaN\n",
      "Name: lifeExp, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 加入 loc 进行切片\n",
    "gapminder = pd.read_csv('../pandas_for_everyone-master/data/gapminder.tsv', sep='\\t')\n",
    "\n",
    "life_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n",
    "y2000 = life_exp[life_exp.index > 2000]\n",
    "print(y2000)\n",
    "\n",
    "# 调用 reindex 方法\n",
    "print(y2000.reindex(range(2000, 2010))) # 此方法相当于重建了新的索引，所以可以展示新索引对应的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                   122\n",
      "Day                    122\n",
      "Cases_Guinea            93\n",
      "Cases_Liberia           83\n",
      "Cases_SierraLeone       87\n",
      "Cases_Nigeria           38\n",
      "Cases_Senegal           25\n",
      "Cases_UnitedStates      18\n",
      "Cases_Spain             16\n",
      "Cases_Mali              12\n",
      "Deaths_Guinea           92\n",
      "Deaths_Liberia          81\n",
      "Deaths_SierraLeone      87\n",
      "Deaths_Nigeria          38\n",
      "Deaths_Senegal          22\n",
      "Deaths_UnitedStates     18\n",
      "Deaths_Spain            16\n",
      "Deaths_Mali             12\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "Date                     0\n",
      "Day                      0\n",
      "Cases_Guinea            29\n",
      "Cases_Liberia           39\n",
      "Cases_SierraLeone       35\n",
      "Cases_Nigeria           84\n",
      "Cases_Senegal           97\n",
      "Cases_UnitedStates     104\n",
      "Cases_Spain            106\n",
      "Cases_Mali             110\n",
      "Deaths_Guinea           30\n",
      "Deaths_Liberia          41\n",
      "Deaths_SierraLeone      35\n",
      "Deaths_Nigeria          84\n",
      "Deaths_Senegal         100\n",
      "Deaths_UnitedStates    104\n",
      "Deaths_Spain           106\n",
      "Deaths_Mali            110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集 country_timeseries.csv\n",
    "ebola = pd.read_csv('../pandas_for_everyone-master/data/country_timeseries.csv')\n",
    "\n",
    "# 统计非缺失值的个数\n",
    "print(ebola.count())\n",
    "print('-' * 50)\n",
    "\n",
    "# 总行数减去不包含缺失值的行数\n",
    "num_rows = ebola.shape[0]\n",
    "num_missing = num_rows - ebola.count()\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# NumPy 统计缺失值\n",
    "import numpy as np\n",
    "\n",
    "ebola = pd.read_csv('../pandas_for_everyone-master/data/country_timeseries.csv')\n",
    "print(np.count_nonzero(ebola.isnull()))                     # 统计缺失值总数\n",
    "print(np.count_nonzero(ebola['Cases_Guinea'].isnull()))     # 统计'Cases_Guinea' 列中缺失值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN       29\n",
      "86.0       3\n",
      "495.0      2\n",
      "112.0      2\n",
      "390.0      2\n",
      "          ..\n",
      "235.0      1\n",
      "231.0      1\n",
      "226.0      1\n",
      "224.0      1\n",
      "2776.0     1\n",
      "Name: Cases_Guinea, Length: 89, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# value_counts 方法获取缺失值个数\n",
    "import numpy as np\n",
    "\n",
    "ebola = pd.read_csv('../pandas_for_everyone-master/data/country_timeseries.csv')\n",
    "print(ebola.Cases_Guinea.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            0.0            10030.0\n",
      "1    1/4/2015  288        2775.0            0.0             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286           0.0         8157.0                0.0\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0            0.0             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0            0.0             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "# 把缺失值重新编码为 0\n",
    "print(ebola.fillna(0).iloc[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            NaN            10030.0\n",
      "1    1/4/2015  288        2775.0            NaN             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286        2769.0         8157.0             9722.0\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0         8018.0             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0         7977.0             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "# 前值填充\n",
    "print(ebola.fillna(method='ffill').iloc[0:10, 0:5]) # ffill fill forward(前值填充)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "117  3/27/2014    5         103.0            8.0                6.0\n",
      "118  3/26/2014    4          86.0            NaN                NaN\n",
      "119  3/25/2014    3          86.0            NaN                NaN\n",
      "120  3/24/2014    2          86.0            NaN                NaN\n",
      "121  3/22/2014    0          49.0            NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "# 后值填充\n",
    "print(ebola.fillna(method='bfill').iloc[:, 0:5].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            NaN            10030.0\n",
      "1    1/4/2015  288        2775.0            NaN             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286        2749.5         8157.0             9677.5\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0         7997.5             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0         7919.5             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "# Pandas插值\n",
    "print(ebola.interpolate().iloc[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 18)\n",
      "(1, 18)\n",
      "          Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone  \\\n",
      "19  11/18/2014  241        2047.0         7082.0             6190.0   \n",
      "\n",
      "    Cases_Nigeria  Cases_Senegal  Cases_UnitedStates  Cases_Spain  Cases_Mali  \\\n",
      "19           20.0            1.0                 4.0          1.0         6.0   \n",
      "\n",
      "    Deaths_Guinea  Deaths_Liberia  Deaths_SierraLeone  Deaths_Nigeria  \\\n",
      "19         1214.0          2963.0              1267.0             8.0   \n",
      "\n",
      "    Deaths_Senegal  Deaths_UnitedStates  Deaths_Spain  Deaths_Mali  \n",
      "19             0.0                  1.0           0.0          6.0  \n"
     ]
    }
   ],
   "source": [
    "# 读取 ebola 数据集的形状\n",
    "print(ebola.shape)\n",
    "\n",
    "# 删除含有 NA 的行或列后得到的数据集形状\n",
    "ebola_dropna = ebola.dropna()\n",
    "print(ebola_dropna.shape)\n",
    "print(ebola_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Cases_Guinea  Cases_Liberia  Cases_SierraLeone  Cases_multiple\n",
      "0          2776.0            NaN            10030.0             NaN\n",
      "1          2775.0            NaN             9780.0             NaN\n",
      "2          2769.0         8166.0             9722.0         20657.0\n",
      "3             NaN         8157.0                NaN             NaN\n",
      "4          2730.0         8115.0             9633.0         20478.0\n",
      "..            ...            ...                ...             ...\n",
      "117         103.0            8.0                6.0           117.0\n",
      "118          86.0            NaN                NaN             NaN\n",
      "119          86.0            NaN                NaN             NaN\n",
      "120          86.0            NaN                NaN             NaN\n",
      "121          49.0            NaN                NaN             NaN\n",
      "\n",
      "[122 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 生成包含病例统计数的新列\n",
    "ebola['Cases_multiple'] = ebola['Cases_Guinea'] + \\\n",
    "                        ebola['Cases_Liberia'] + \\\n",
    "                        ebola['Cases_SierraLeone']\n",
    "\n",
    "# 查看前10行的计算结果\n",
    "ebola_subset = ebola.loc[:, ['Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone', 'Cases_multiple']]\n",
    "print(ebola_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 6 章：整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k  \\\n",
      "0                  Agnostic     27       34       60       81       76   \n",
      "1                   Atheist     12       27       37       52       35   \n",
      "2                  Buddhist     27       21       30       34       33   \n",
      "3                  Catholic    418      617      732      670      638   \n",
      "4        Don’t know/refused     15       14       15       11       10   \n",
      "5          Evangelical Prot    575      869     1064      982      881   \n",
      "6                     Hindu      1        9        7        9       11   \n",
      "7   Historically Black Prot    228      244      236      238      197   \n",
      "8         Jehovah's Witness     20       27       24       24       21   \n",
      "9                    Jewish     19       19       25       25       30   \n",
      "10            Mainline Prot    289      495      619      655      651   \n",
      "11                   Mormon     29       40       48       51       56   \n",
      "12                   Muslim      6        7        9       10        9   \n",
      "13                 Orthodox     13       17       23       32       32   \n",
      "14          Other Christian      9        7       11       13       13   \n",
      "15             Other Faiths     20       33       40       46       49   \n",
      "16    Other World Religions      5        2        3        4        2   \n",
      "17             Unaffiliated    217      299      374      365      341   \n",
      "\n",
      "    $50-75k  $75-100k  $100-150k  >150k  Don't know/refused  \n",
      "0       137       122        109     84                  96  \n",
      "1        70        73         59     74                  76  \n",
      "2        58        62         39     53                  54  \n",
      "3      1116       949        792    633                1489  \n",
      "4        35        21         17     18                 116  \n",
      "5      1486       949        723    414                1529  \n",
      "6        34        47         48     54                  37  \n",
      "7       223       131         81     78                 339  \n",
      "8        30        15         11      6                  37  \n",
      "9        95        69         87    151                 162  \n",
      "10     1107       939        753    634                1328  \n",
      "11      112        85         49     42                  69  \n",
      "12       23        16          8      6                  22  \n",
      "13       47        38         42     46                  73  \n",
      "14       14        18         14     12                  18  \n",
      "15       63        46         40     41                  71  \n",
      "16        7         3          4      4                   8  \n",
      "17      528       407        321    258                 597  \n"
     ]
    }
   ],
   "source": [
    "# 导入美国收入和宗教信仰数据\n",
    "import pandas as pd\n",
    "\n",
    "pew = pd.read_csv('../pandas_for_everyone-master/data/pew.csv')\n",
    "print(pew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion variable  value\n",
      "0            Agnostic    <$10k     27\n",
      "1             Atheist    <$10k     12\n",
      "2            Buddhist    <$10k     27\n",
      "3            Catholic    <$10k    418\n",
      "4  Don’t know/refused    <$10k     15\n",
      "                  religion            variable  value\n",
      "175               Orthodox  Don't know/refused     73\n",
      "176        Other Christian  Don't know/refused     18\n",
      "177           Other Faiths  Don't know/refused     71\n",
      "178  Other World Religions  Don't know/refused      8\n",
      "179           Unaffiliated  Don't know/refused    597\n",
      "--------------------------------------------------\n",
      "             religion income  count\n",
      "0            Agnostic  <$10k     27\n",
      "1             Atheist  <$10k     12\n",
      "2            Buddhist  <$10k     27\n",
      "3            Catholic  <$10k    418\n",
      "4  Don’t know/refused  <$10k     15\n",
      "                  religion              income  count\n",
      "175               Orthodox  Don't know/refused     73\n",
      "176        Other Christian  Don't know/refused     18\n",
      "177           Other Faiths  Don't know/refused     71\n",
      "178  Other World Religions  Don't know/refused      8\n",
      "179           Unaffiliated  Don't know/refused    597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pew = pd.read_csv('../pandas_for_everyone-master/data/pew.csv')\n",
    "\n",
    "# 不指定 value_vars，因为想对除“religion”列以外的所有列进行透视\n",
    "pew_long = pd.melt(pew, id_vars='religion')\n",
    "print(pew_long.head())  # 打印前5行\n",
    "print(pew_long.tail())  # 打印后5行\n",
    "print('-' * 50)\n",
    "\n",
    "# 更改默认值，以便命名进行融合、逆透视的列\n",
    "pew_long = pd.melt(pew, id_vars='religion', var_name='income', value_name='count')\n",
    "print(pew_long.head())\n",
    "print(pew_long.tail()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered  wk1   wk2  \\\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   87  82.0   \n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   91  87.0   \n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08   81  70.0   \n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21   76  76.0   \n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15   57  34.0   \n",
      "\n",
      "    wk3   wk4   wk5  ...  wk67  wk68  wk69  wk70  wk71  wk72  wk73  wk74  \\\n",
      "0  72.0  77.0  87.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1  92.0   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2  68.0  67.0  66.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "3  72.0  69.0  67.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "4  25.0  17.0  17.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "   wk75  wk76  \n",
      "0   NaN   NaN  \n",
      "1   NaN   NaN  \n",
      "2   NaN   NaN  \n",
      "3   NaN   NaN  \n",
      "4   NaN   NaN  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "--------------------------------------------------\n",
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n",
      "       year            artist                    track  time date.entered  \\\n",
      "24087  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   \n",
      "24088  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   \n",
      "24089  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   \n",
      "24090  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   \n",
      "24091  2000   matchbox twenty                     Bent  4:12   2000-04-29   \n",
      "\n",
      "       week  rating  \n",
      "24087  wk76     NaN  \n",
      "24088  wk76     NaN  \n",
      "24089  wk76     NaN  \n",
      "24090  wk76     NaN  \n",
      "24091  wk76     NaN  \n"
     ]
    }
   ],
   "source": [
    "# 固定多列\n",
    "# 导入数据集 billboard\n",
    "import pandas as pd\n",
    "\n",
    "billboard = pd.read_csv('../pandas_for_everyone-master/data/billboard.csv')\n",
    "print(billboard.head())\n",
    "print('-' * 50)\n",
    "\n",
    "billboard_long = pd.melt(\n",
    "                billboard,                                                          # 数据集为 billboard\n",
    "                id_vars=['year', 'artist', 'track', 'time', 'date.entered'],        # 需要固定的多个列\n",
    "                var_name='week',                                                    # 逆透视的列融合而成的数据，列名为 week\n",
    "                value_name='rating'                                                 # 放置逆透视融合数据的值的组成的列，列名为 rating\n",
    "                )\n",
    "print(billboard_long.head())\n",
    "print(billboard_long.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',\n",
      "       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',\n",
      "       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',\n",
      "       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',\n",
      "       'Deaths_Spain', 'Deaths_Mali'],\n",
      "      dtype='object')\n",
      "         Date  Day  Cases_Guinea  Cases_Liberia  Deaths_Guinea  Deaths_Liberia\n",
      "0    1/5/2015  289        2776.0            NaN         1786.0             NaN\n",
      "1    1/4/2015  288        2775.0            NaN         1781.0             NaN\n",
      "2    1/3/2015  287        2769.0         8166.0         1767.0          3496.0\n",
      "3    1/2/2015  286           NaN         8157.0            NaN          3496.0\n",
      "4  12/31/2014  284        2730.0         8115.0         1739.0          3471.0\n"
     ]
    }
   ],
   "source": [
    "# 包含多个变量的列\n",
    "import pandas as pd\n",
    "\n",
    "ebola = pd.read_csv('../pandas_for_everyone-master/data/country_timeseries.csv')\n",
    "print(ebola.columns)        # 查看数据集 ebola 的字段名（列名）\n",
    "print(ebola.iloc[:5, [0, 1, 2, 3, 10, 11]]) # 查看数据集的前5行，及选定的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0\n",
      "3    1/2/2015  286  Cases_Guinea     NaN\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0\n",
      "           Date  Day     variable  value\n",
      "1947  3/27/2014    5  Deaths_Mali    NaN\n",
      "1948  3/26/2014    4  Deaths_Mali    NaN\n",
      "1949  3/25/2014    3  Deaths_Mali    NaN\n",
      "1950  3/24/2014    2  Deaths_Mali    NaN\n",
      "1951  3/22/2014    0  Deaths_Mali    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ebola = pd.read_csv('../pandas_for_everyone-master/data/country_timeseries.csv')\n",
    "ebola_long = pd.melt(ebola, id_vars=['Date', 'Day'])    # 对数据集进行逆透视\n",
    "print(ebola_long.head())\n",
    "print(ebola_long.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Cases, Guinea]\n",
      "1    [Cases, Guinea]\n",
      "2    [Cases, Guinea]\n",
      "3    [Cases, Guinea]\n",
      "4    [Cases, Guinea]\n",
      "Name: variable, dtype: object\n",
      "1947    [Deaths, Mali]\n",
      "1948    [Deaths, Mali]\n",
      "1949    [Deaths, Mali]\n",
      "1950    [Deaths, Mali]\n",
      "1951    [Deaths, Mali]\n",
      "Name: variable, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 获取 variable 列\n",
    "# 访问字符串方法\n",
    "# 依据分隔符拆分列\n",
    "variable_split = ebola_long.variable.str.split('_')     # 将 variable 列按照分隔符（_）进行拆分\n",
    "print(variable_split[:5])                               # 打印拆分后的列的前5行\n",
    "print(variable_split[-5:])                              # 打印拆分后的列的后5行\n",
    "\n",
    "print(type(variable_split))                             # 打印拆分后的列的数据类型\n",
    "print(type(variable_split[0]))                          # 打印容器中的第一个元素的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Cases\n",
      "1    Cases\n",
      "2    Cases\n",
      "3    Cases\n",
      "4    Cases\n",
      "Name: variable, dtype: object\n",
      "1947    Deaths\n",
      "1948    Deaths\n",
      "1949    Deaths\n",
      "1950    Deaths\n",
      "1951    Deaths\n",
      "Name: variable, dtype: object\n",
      "0    Guinea\n",
      "1    Guinea\n",
      "2    Guinea\n",
      "3    Guinea\n",
      "4    Guinea\n",
      "Name: variable, dtype: object\n",
      "1947    Mali\n",
      "1948    Mali\n",
      "1949    Mali\n",
      "1950    Mali\n",
      "1951    Mali\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 提取拆分列得到的部分\n",
    "status_values = variable_split.str.get(0)\n",
    "country_values = variable_split.str.get(1)\n",
    "\n",
    "print(status_values[:5])\n",
    "print(status_values[-5:])\n",
    "\n",
    "print(country_values[:5])\n",
    "print(country_values[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "# 将所得部分赋值到新列\n",
    "ebola_long['status'] = status_values\n",
    "ebola_long['country'] = country_values\n",
    "\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea  Cases  Guinea\n",
      "           Date  Day     variable  value  status country  status country\n",
      "1947  3/27/2014    5  Deaths_Mali    NaN  Deaths    Mali  Deaths    Mali\n",
      "1948  3/26/2014    4  Deaths_Mali    NaN  Deaths    Mali  Deaths    Mali\n",
      "1949  3/25/2014    3  Deaths_Mali    NaN  Deaths    Mali  Deaths    Mali\n",
      "1950  3/24/2014    2  Deaths_Mali    NaN  Deaths    Mali  Deaths    Mali\n",
      "1951  3/22/2014    0  Deaths_Mali    NaN  Deaths    Mali  Deaths    Mali\n"
     ]
    }
   ],
   "source": [
    "# 在单个步骤中进行拆分和组合\n",
    "# 因为拆分组合的2次，所以有2个status和country列\n",
    "\n",
    "variable_split = ebola_long.variable.str.split('_', expand=True)\n",
    "variable_split.columns = ['status', 'country']\n",
    "ebola_parsed = pd.concat([ebola_long, variable_split], axis=1)\n",
    "print(ebola_parsed.head())\n",
    "print(ebola_parsed.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pi', '3.14'), ('e', '2.718')]\n"
     ]
    }
   ],
   "source": [
    "# zip函数示例\n",
    "contants = ['pi', 'e']\n",
    "values = ['3.14', '2.718']\n",
    "\n",
    "# 为了显示 zip 对象的内容，必须使用列表把 zip 函数包裹起来\n",
    "# 在 Python3 中，zip 函数返回一个迭代器\n",
    "print(list(zip(contants, values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "ebola_long['status'], ebola_long['country'] = zip(*ebola_long.variable.str.split('_'))\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element  d1    d2    d3  d4    d5  d6  d7\n",
      "0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN NaN\n",
      "1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN NaN\n",
      "2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN NaN\n",
      "3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN NaN\n",
      "4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN NaN\n"
     ]
    }
   ],
   "source": [
    "# 查看weather数据集\n",
    "weather = pd.read_csv('../pandas_for_everyone-master/data/weather.csv')\n",
    "print(weather.iloc[:5, :11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element day  temp\n",
      "0  MX17004  2010      1    tmax  d1   NaN\n",
      "1  MX17004  2010      1    tmin  d1   NaN\n",
      "2  MX17004  2010      2    tmax  d1   NaN\n",
      "3  MX17004  2010      2    tmin  d1   NaN\n",
      "4  MX17004  2010      3    tmax  d1   NaN\n",
      "          id  year  month element  day  temp\n",
      "677  MX17004  2010     10    tmin  d31   NaN\n",
      "678  MX17004  2010     11    tmax  d31   NaN\n",
      "679  MX17004  2010     11    tmin  d31   NaN\n",
      "680  MX17004  2010     12    tmax  d31   NaN\n",
      "681  MX17004  2010     12    tmin  d31   NaN\n",
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n",
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n"
     ]
    }
   ],
   "source": [
    "# 首先对 day 值进行融合/逆透视\n",
    "weather_melt = pd.melt(weather, \n",
    "                id_vars=['id', 'year', 'month', 'element'],\n",
    "                var_name='day',\n",
    "                value_name='temp')\n",
    "print(weather_melt.head())\n",
    "print(weather_melt.tail())\n",
    "\n",
    "# 将 element 列在存储的变量展开，类似透视过程\n",
    "weather_tidy = weather_melt.pivot_table(\n",
    "                    index=['id', 'year', 'month', 'day'], \n",
    "                    columns='element',\n",
    "                    values='temp'\n",
    ")\n",
    "weather_tidy_flat = weather_tidy.reset_index()\n",
    "print(weather_tidy_flat.head())\n",
    "\n",
    "# 可以不使用中间 DataFrame 而直接使用此方法\n",
    "weather_tidy = weather_melt.pivot_table(\n",
    "            index=['id', 'year', 'month', 'day'],\n",
    "            columns='element',\n",
    "            values='temp').reset_index()\n",
    "print(weather_tidy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n",
      "      year        artist  track  time date.entered week  rating\n",
      "3     2000  3 Doors Down  Loser  4:24   2000-10-21  wk1    76.0\n",
      "320   2000  3 Doors Down  Loser  4:24   2000-10-21  wk2    76.0\n",
      "637   2000  3 Doors Down  Loser  4:24   2000-10-21  wk3    72.0\n",
      "954   2000  3 Doors Down  Loser  4:24   2000-10-21  wk4    69.0\n",
      "1271  2000  3 Doors Down  Loser  4:24   2000-10-21  wk5    67.0\n"
     ]
    }
   ],
   "source": [
    "# 以 billboard 数据集为例\n",
    "print(billboard_long.head())\n",
    "print(billboard_long[billboard_long.track == 'Loser'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 4)\n",
      "(317, 4)\n",
      "   year          artist                    track  time  id\n",
      "0  2000           2 Pac  Baby Don't Cry (Keep...  4:22   0\n",
      "1  2000         2Ge+her  The Hardest Part Of ...  3:15   1\n",
      "2  2000    3 Doors Down               Kryptonite  3:53   2\n",
      "3  2000    3 Doors Down                    Loser  4:24   3\n",
      "4  2000        504 Boyz            Wobble Wobble  3:35   4\n",
      "5  2000            98^0  Give Me Just One Nig...  3:24   5\n",
      "6  2000         A*Teens            Dancing Queen  3:44   6\n",
      "7  2000         Aaliyah            I Don't Wanna  4:15   7\n",
      "8  2000         Aaliyah                Try Again  4:03   8\n",
      "9  2000  Adams, Yolanda            Open My Heart  5:30   9\n"
     ]
    }
   ],
   "source": [
    "# 获取单独的歌曲 DataFrame\n",
    "billboard_songs = billboard_long[['year', 'artist', 'track', 'time']]\n",
    "print(billboard_songs.shape)\n",
    "\n",
    "billboard_songs = billboard_songs.drop_duplicates() # 删除数据集中的重复行\n",
    "print(billboard_songs.shape)        # 对比形状的变化，可知不重复的行仅有317行，而总行数有24092行\n",
    "\n",
    "billboard_songs['id'] = range(len(billboard_songs)) # 根据行数分配唯一的id，与SQL中默认id相似，但以0开始\n",
    "print(billboard_songs.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 8)\n",
      "   year artist                    track  time date.entered week  rating  id\n",
      "0  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0   0\n",
      "1  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk2    82.0   0\n",
      "2  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk3    72.0   0\n",
      "3  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk4    77.0   0\n",
      "4  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk5    87.0   0\n",
      "   id date.entered week  rating\n",
      "0   0   2000-02-26  wk1    87.0\n",
      "1   0   2000-02-26  wk2    82.0\n",
      "2   0   2000-02-26  wk3    72.0\n",
      "3   0   2000-02-26  wk4    77.0\n",
      "4   0   2000-02-26  wk5    87.0\n"
     ]
    }
   ],
   "source": [
    "# 把歌曲 DataFrame 合并到原数据集\n",
    "billboard_ratings = billboard_long.merge(\n",
    "                    billboard_songs, on=['year', 'artist', 'track', 'time'])\n",
    "print(billboard_ratings.shape)\n",
    "print(billboard_ratings.head())\n",
    "\n",
    "# 取出想要放入排行 DataFrame 中的列子集\n",
    "billboard_ratings = billboard_ratings[['id', 'date.entered', 'week', 'rating']]\n",
    "print(billboard_ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-01.csv\n",
      "\n",
      "..\\pandas_for_everyone-master\\data\\fhv_tripdata_2015-01.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-02.csv\n",
      "\n",
      "..\\pandas_for_everyone-master\\data\\fhv_tripdata_2015-02.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-03.csv\n",
      "\n",
      "..\\pandas_for_everyone-master\\data\\fhv_tripdata_2015-03.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-04.csv\n",
      "\n",
      "..\\pandas_for_everyone-master\\data\\fhv_tripdata_2015-04.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-05.csv\n",
      "\n",
      "..\\pandas_for_everyone-master\\data\\fhv_tripdata_2015-05.csv\n"
     ]
    }
   ],
   "source": [
    "# 数据下载代码\n",
    "# 只下载前5个数据集\n",
    "import os, urllib\n",
    "\n",
    "with open('../pandas_for_everyone-master/data/raw_data_urls.txt', 'r') as data_urls:\n",
    "    for line, url in enumerate(data_urls):\n",
    "        if line == 5:\n",
    "            break\n",
    "        fn = url.split('/')[-1].strip()\n",
    "        fp = os.path.join('..', 'pandas_for_everyone-master', 'data', fn)\n",
    "        print(url)\n",
    "        print(fp)\n",
    "        urllib.request.urlretrieve(url, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../pandas_for_everyone-master/data\\\\fhv_tripdata_2015-01.csv',\n",
      " '../pandas_for_everyone-master/data\\\\fhv_tripdata_2015-02.csv',\n",
      " '../pandas_for_everyone-master/data\\\\fhv_tripdata_2015-03.csv',\n",
      " '../pandas_for_everyone-master/data\\\\fhv_tripdata_2015-04.csv',\n",
      " '../pandas_for_everyone-master/data\\\\fhv_tripdata_2015-05.csv']\n",
      "查看各数据集: \n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-02-01 00:00:00         NaN\n",
      "1               B00013  2015-02-01 00:01:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00029  2015-03-01 00:02:00       213.0\n",
      "1               B00029  2015-03-01 00:03:00        51.0\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-04-01 04:30:00         NaN\n",
      "1               B00001  2015-04-01 06:00:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-05-01 04:30:00         NaN\n",
      "1               B00001  2015-05-01 05:00:00         NaN\n",
      "查看各数据集shape: \n",
      "(2746033, 3)\n",
      "(3126401, 3)\n",
      "(3281427, 3)\n",
      "(3917789, 3)\n",
      "(4296067, 3)\n",
      "连接 DataFrame\n",
      "(17367717, 3)\n"
     ]
    }
   ],
   "source": [
    "# 获取要加载的文件名列表\n",
    "import glob, pprint\n",
    "\n",
    "# 从 nyc_taxi 数据文件夹获得 csv 文件列表\n",
    "nyc_taxi_data = glob.glob('../pandas_for_everyone-master/data/fhv_*')\n",
    "pprint.pprint(nyc_taxi_data)\n",
    "\n",
    "# 把这些文件都加载到 DataFrame 中\n",
    "taxi1 = pd.read_csv(nyc_taxi_data[0])\n",
    "taxi2 = pd.read_csv(nyc_taxi_data[1])\n",
    "taxi3 = pd.read_csv(nyc_taxi_data[2])\n",
    "taxi4 = pd.read_csv(nyc_taxi_data[3])\n",
    "taxi5 = pd.read_csv(nyc_taxi_data[4])\n",
    "\n",
    "print('查看各数据集: ')\n",
    "print(taxi1.head(n=2))\n",
    "print(taxi2.head(n=2))\n",
    "print(taxi3.head(n=2))\n",
    "print(taxi4.head(n=2))\n",
    "print(taxi5.head(n=2))\n",
    "\n",
    "print('查看各数据集shape: ')\n",
    "print(taxi1.shape)\n",
    "print(taxi2.shape)\n",
    "print(taxi3.shape)\n",
    "print(taxi4.shape)\n",
    "print(taxi5.shape)\n",
    "\n",
    "print('连接 DataFrame')\n",
    "taxi = pd.concat([taxi1, taxi2, taxi3, taxi4, taxi5])\n",
    "print(taxi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pandas_for_everyone-master/data\\fhv_tripdata_2015-01.csv\n",
      "../pandas_for_everyone-master/data\\fhv_tripdata_2015-02.csv\n",
      "../pandas_for_everyone-master/data\\fhv_tripdata_2015-03.csv\n",
      "../pandas_for_everyone-master/data\\fhv_tripdata_2015-04.csv\n",
      "../pandas_for_everyone-master/data\\fhv_tripdata_2015-05.csv\n",
      "5\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "2               B00013  2015-01-01 01:23:00         NaN\n",
      "3               B00013  2015-01-01 01:44:00         NaN\n",
      "4               B00013  2015-01-01 02:00:00         NaN\n",
      "(17367717, 3)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 创建一个待添加元素的空列表\n",
    "list_taxi_df = []\n",
    "\n",
    "# 循环遍历每个 CSV 文件名\n",
    "for csv_filename in nyc_taxi_data:\n",
    "    # 可以选择输出文件名以便调试\n",
    "    print(csv_filename)\n",
    "\n",
    "    # 把 CSV 文件加载到 DataFrame 中\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    # 把 DataFrame 添加到列表中\n",
    "    list_taxi_df.append(df)\n",
    "\n",
    "# 列表中 DataFrame 的个数\n",
    "print(len(list_taxi_df))\n",
    "\n",
    "# 第一个元素的类型\n",
    "print(type(list_taxi_df[0]))\n",
    "\n",
    "# 查看第一个DataFrame的前5行数据\n",
    "print(list_taxi_df[0].head())\n",
    "\n",
    "# 将列表中的 DataFrame 连接在一起\n",
    "taxi_loop_concat = pd.concat(list_taxi_df)\n",
    "print(taxi_loop_concat.shape)\n",
    "\n",
    "# 判断手动加载连接与循环加载连接得到的结果是否一致\n",
    "print(taxi.equals(taxi_loop_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 使用列表推导式重写\n",
    "list_taxi_df_comp = [pd.read_csv(data) for data in nyc_taxi_data]\n",
    "print(type(list_taxi_df_comp))\n",
    "taxi_loop_concat_comp = pd.concat(list_taxi_df_comp)\n",
    "\n",
    "# 最终结果是否一致\n",
    "print(taxi_loop_concat_comp.equals(taxi_loop_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 9, 12, 15, 18, 21, 24, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp/ipykernel_15112/2433538309.py:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  multiples = [i for i in range(30) if i % 3 is 0]\n"
     ]
    }
   ],
   "source": [
    "# 列表推导式示例\n",
    "multiples = [i for i in range(30) if i % 3 is 0]\n",
    "print(multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# for 循环生成新列表\n",
    "squared1 = []\n",
    "for x in range(10):\n",
    "    squared1.append(x**2)\n",
    "print(squared1)\n",
    "\n",
    "# 列表推导式生成新列表\n",
    "squared2 = [x**2 for x in range(10)]\n",
    "print(squared2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21fd37b01094b8e9106f03c9b3796a4204c380be4188143ffe484c301302cdcd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
