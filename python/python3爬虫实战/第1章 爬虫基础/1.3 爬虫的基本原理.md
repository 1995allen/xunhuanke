# 1.3 爬虫的基本原理
## 1.3.1 爬虫概述
==爬虫就是获取网页并提取和保存信息的自动化程序。==
### 获取网页
- 最关键部分是构造一个请求并发送给服务器，然后接收到响应并对其进行解析。
- 得到响应之后只需要解析数据结构中的 body 部分，即可得到网页的源代码，这样我们就可以用程序来实现获取网页的过程。
### 提取信息
- 获取网页的源代码后，接下来就是分析源代码，从中提取我们想要的数据。
- 提取信息是爬虫非常重要的一个工作，它可以使杂乱的数据变得条理清晰，以便后续处理和分析数据。
### 保存数据
- 提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。
### 自动化程序
- 自动化程序的意思是爬虫可以代替人来完成上述操作。
## 1.3.2 能爬怎样的数据
1. 最常见的便是常规网页，这些网页对应着 HTML 代码，而最常抓取的就是 HTML 源代码。
2. 另外，有些网页可能返回一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析。
3. 网页中还包含各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存对应的文件名。
4. 网页中还有各种扩展名文件，如 CSS、JavaScript 和配置文件等
> 上述内容其实都有各自对应的 URL，URL 基于 HTTP 或 HTTPS 协议，只要是这种数据，爬虫都可以抓取
## 1.3.3 JavaScript 渲染的页面
- 例：现在有越来越多的网页是采用 Ajax、前端模块化工具构建的，可能整个网页都是由 JavaScript 渲染出来的，也就是说原始的 HTML 代码是一个空壳。
---
``` html
  <!DOCTYPE html>
  <html>
        <head>
          <meta charst="utf-8">
          <title>This is a Demo</title>
        <head>
        <body>
            <div id="container">
            </div>
        </body>
  <script src="app.js"></script>
  </html>
```
- 这个实例中，body 节点里面只有一个 id 为 container 的节点，需要注意在 body 节点后引入了 app.j，它负责整个网站的渲染。
- 在浏览器中打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中引入了一个 app.js 文件，便去请求这个文件。
- 获取该文件后，执行其中的 JavaScript 代码，JavaScript 会改变 HTML 在的节点，向其中添加内容，最后得到完整的页面。
- 对于这样的情况，我们可以分析源代码后台 Ajax  接口，也可使用 Selenium、Splash、Pyppeteer、Playwright 这样的库来模拟  JavaScript 渲染。
## 1.3.4 总结
> 介绍了爬虫的一些基本原理，熟知这些原理可以使我们在后面编写爬虫时更加得心应手。